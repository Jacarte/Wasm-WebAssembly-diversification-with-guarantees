%%
%% This is file `sample-sigplan.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigplan')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 

%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigplan.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigplan,screen]{acmart}
\usepackage{algorithm} 
\usepackage[noend]{algpseudocode} 
\usepackage[titles]{tocloft}
\usepackage{tikz}
\usepackage{listings}
\usepackage{xspace}
\usepackage{tcolorbox}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage[inline]{enumitem}
\usepackage{xspace}
\usepackage[listings,skins]{tcolorbox}
%\usepackage{subcaption}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


\makeatletter


\newenvironment{btHighlight}[1][]
{\begingroup\tikzset{bt@Highlight@par/.style={#1}}\begin{lrbox}{\@tempboxa}}
{\end{lrbox}\bt@HL@box[bt@Highlight@par]{\@tempboxa}\endgroup}


\definecolor{commentgreen}{RGB}{176, 176, 176}
\definecolor{rowcolor}{cmyk}{0,0.87,0.68,0.32}
\definecolor{rowcolor2}{cmyk}{ 20, 0, 37, 34}

\definecolor{eminence}{RGB}{108,48,130}
\definecolor{weborange}{RGB}{255,165,0}
\definecolor{frenchplum}{RGB}{129,20,82}
\definecolor{darkgreen}{RGB}{10, 92, 10}

\definecolor{celadon}{rgb}{0.67, 0.88, 0.69}
%\renewcommand{\blue}{}


% Subfigures caption


\newcommand\btHL[1][]{%
  \begin{btHighlight}[#1]\bgroup\aftergroup\bt@HL@endenv%
}
\def\bt@HL@endenv{%
  \end{btHighlight}%   
  \egroup
}
\newcommand{\bt@HL@box}[2][]{%
  \tikz[#1]{%
    \pgfpathrectangle{\pgfpoint{1pt}{0pt}}{\pgfpoint{\wd #2}{\ht #2}}%
    \pgfusepath{use as bounding box}%
    \node[anchor=base west, fill=orange!30,outer sep=0pt,inner xsep=1pt, inner ysep=0pt, rounded corners=3pt, minimum height=\ht\strutbox+1pt,#1]{\raisebox{1pt}{\strut}\strut\usebox{#2}};
  }%
}
\makeatother
\newcommand*\badge[1]{ \colorbox{red}{\color{white}#1}}
\newcommand{\tool}{Wasm-mutate\xspace}
\newcommand{\wasm}{Wasm\xspace}
\newcommand{\Wasm}{WebAssembly\xspace}
\newcommand{\etal}{et.al.\xspace}
\newcommand{\ie}{i.e.,\xspace}

\newtheorem{definition}{Definition}
\providecommand*{\definitionautorefname}{Definition}
\newtheorem{metric}{Metric}
\providecommand*{\metricautorefname}{Metric}

\newlistof{todo}{td}{List of TODOs}

\newcommand{\revision}[1]{{\textcolor{red}{#1}}}
\newcommand{\repourl}{\url{https://github.com/Jacarte/wasm_evasion}}

\newcommand{\todo}[1]{%
\refstepcounter{todo}
\noindent\textbf{\badge{TODO}} {\color{red}#1}
\addcontentsline{td}{todo}
{\color{red}\thesection.\thetodo\xspace #1}}

\input{listings_config.tex}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{WebAssembly Binary Diversification with Guarantees}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.

\section{Introduction}

\todo{P 1 and 2 too long and general.}
\todo{The first part can be shorterd}

% Sota of Wasm
\Wasm has earned its place as the fourth official language of the Web \cite{WebAssemblyCoreSpecification}.
This language offers low-level constructs that facilitate efficient execution times, bridging the gap between it and native code, significantly more than JavaScript \cite{haas2017bringing}.
As a result, \wasm has seen a swift rise in adoption since its debut in 2015.
At present, languages such as Rust and C/C++ can be compiled to \wasm using established toolchains and can be executed in all major browsers.
A noteworthy facet of its adoption is its adaptability for use outside of browsers.
For example, Function as a Service platforms like Fastly \cite{fastly} utilize WebAssembly as their foundational technology.
\todo{Short here before}
While LLVM is responsible for 70\% of generated \Wasm in the wild in 2021 \cite{hilbig2021empirical}, the community has observed the emergence of new pipelines that output \wasm binaries.
For instance, the recently introduced Javy compiler \cite{javy} generates \wasm programs from JavaScript code using a methodology that is entirely distinct from LLVM.
A similar case recently appears for Kotlin through the JVM \cite{kmm}.
\todo{Wasm to Wasm is better than hacking through compilation pipeline. This was done in LLVM beacuase it accounted for 70\% of the generated programs, yet this night not be real anymore, so wasm to was is imperative.}


\todo{Second pargraph stating that we do diversification.}

% Security issues, CVE, how diversification plays
The \wasm execution model is meticulously designed to be secure, aiming to mitigate a broad spectrum of memory and control flow attacks.
However, as acknowledged in its official documentation \cite{WebAssemblySecurity}, \wasm is not invulnerable to potential exploitations, neither the tools that generate \Wasm programs \cite{usenixWASM2020, stievenart2021security}.
On the flip side, code diversification is a significant proactive that notably enforces security by automatic testing and perturbing security sensible side-channels \cite{arteaga2020crow, MEWE, CABRERAARTEAGA2023103296}.
Code diversification entails generating varied variants of an original program that provide the same functionalities but exhibit different execution behaviours \cite{cohen1993operating}.
This can be accomplished by applying code transformations to the original program.
Essentially, the application of semantically equivalent transformations to a program could aid in preventing security issues.
For instance, a CVE in Fastly was identified in 2021 \cite{CVE} by applying semantically equivalent transformations to a deployed binary, underscoring the benefits of this technique for testing.
However, existing diversification techniques are primarily tailored for LLVM compilers, missing potential places in which code diversification can be achieved.
Therefore, as previously mentioned, the emerging technologies that generate \wasm binaries may not fully benefit from these techniques.
As a result, the development of \Wasm to \Wasm diversification tools becomes imperative.
This work presents a new software diversification tool for \Wasm, \tool.
\todo{Why is good to have many variants in just a few minutes. Good for testing and fuzzing.}


% Now, what is new in tool, what does it offer
\tool is a dedicated software diversification tool for \Wasm.
It is agnostic to compilers and operates directly over \Wasm programs by using code rewriting rules.
It can be utilized for testing \wasm compilers and other \wasm-related tools such as validators or interpreters.
Given a valid Wasm input program, \tool can generate thousands of different behavioural \wasm programs within minutes that are semantically equivalent to the original input.
To the best of our knowledge, it is the first approach that models the search space for new programs variants as an e-graph \cite{10.1145/3434304}, leveraging the property that any traversal through the e-graph represents a semantically equivalent variant of the input program.
\todo{Extend, focus on guarantees.}
\todo{We do many transformations. Our manucally crafted transformations follows Cohen. THe variants guarantees to preserve seemantics.
\tool can generate thoursands of variants in a few minutes, which is good for testing, evasion and protection.
In this paper we evaluate the first two, testing and protection.}

\todo{Talks about the CVE later.}


\todo{Next: talk about the protocol. How do we check behavioral diversity}
% Evaluation and hits to results
We assess the efficacy of \tool by determining the number of unique and behaviorally distinct variants it produces from the same corpus previously utilized by the cutting-edge software diversifier for \Wasm, CROW \cite{arteaga2020crow}. 
Our evaluation shows that \tool is capable of generating thousands of variants in a mere hour.
Notably, it also introduces substantial alterations in the execution traces of these variants. 
Our experiments further illustrate that \tool successfully generates variants with high preservation rates. 
In other words, these variants retain their unique characteristics when compiled to machine code by the wasmtime JIT compiler, cranelift.

\todo{Be declarative: we can generate diversify, we generate diversify at runtime, this diversity is good at protecting.}
\todo{Another paragraph about the dataset. We reuse the one from CROW, we use the ine from Swivel. Interchange LLVM-based diversification with our tool.}
\todo{One or two key results.}
% Security :)
Significantly, we carry out a series of experiments where \tool is utilized to generate diversified versions of \wasm programs previously employed in Swivel \cite{Swivel} known for being susceptible to Spectre attacks.
Our findings indicate that these diversified versions demonstrate resilience against such attacks. 
This outcome emphasizes the potential of \tool as a practical instrument for strengthening the security against side-channel attacks for \wasm programs. 
It suggests that \tool can serve as an effective strategy in the ongoing fight against Spectre and similar attacks, offering an additional layer of security that enhances existing safeguards.
Moreover, these results also underscore the wider implications of our work. 
By showcasing the effectiveness of \tool in a real-world scenario, we offer valuable insights that could steer future research and development in the realm of software diversification for \Wasm. 
This could lead to the creation of more robust and secure \Wasm applications, further contributing to the field's advancement.
% Why diversification.


%Code diversification \cite{larsen2014sok,baudry2015multiple} offers an additional layer of protection that can fortify the \wasm stack.
%This involves generating diverse variants of an original program that deliver the same functionalities but display different execution traces.
%In this paper, we explore the feasibility of diversifying \wasm code, a topic that, to the best of our knowledge, remains unexplored.





%- Wasm-mutate is a completement for wasm-smith and they must be used together. For example, wasm-mutate is not able to create loops from scratch, while wasm-smith can. Since the need is speed, each implementation is done as fast as possible. This relies on the backend implementation of code generation to optimize and generate performance code.

%- Ephasize that Wasm 2 Wasm is better.  The limitations for CROW and MEWE. They are based on LLVM. They use a theorem solver, which is extremely slow. The same theorem solver could be used to curate the rewriting rules. LLVM onlu accounts for 70\% of the Wasm binaries in the wild. Now we have Javy and other compilers support Wasm. Wasm 2 wasm solution is impeorative.


To sum up, the contributions of this work are:

\begin{itemize}
    \item \tool, a tool tailored for WebAssembly to WebAssembly software diversification.
    \item Empirical evidence on static and dynamic differences provided by the variants created by \tool.
    \item Empirical evidence on how \tool can protect cache timing side-channel attacks, specifically, Spectre.
    
\end{itemize}

This paper is structured as follows. 
In \autoref{background}, we introduce WebAssembly, the concepts of semantic equivalence and what we state as a rewriting rule.
In \autoref{tech}, we explain and detail the architecture and implementation of \tool.
We formulate our research questions in \autoref{eval}, answering them in \autoref{results}.
We discuss open challenges related to our research in \autoref{discussion}, in order to help future research projects on similar topics.
In \autoref{rw} we highlight works related to our research on software diversification.
We finalize with our conclusions \autoref{conc}.

\section{Background}
\label{background}

In this section, we define and formulate the foundation of this work: WebAssembly, its runtime structure, semantic equivalence modulo input, rewriting rules and e-graphs.
Along with the paper, we use the terms, metrics and concepts defined here.

\subsection{WebAssembly}

WebAssembly (Wasm) is a binary instruction set initially  meant for the web, and now also used in the backend. 
It was adopted as a standardized language by the W3C in 2017, building upon the work of Haas et al. \cite{haas2017bringing}. One of Wasm's primary advantages is that it defines its own Instruction Set Architecture (ISA), which is both platform-independent. As a result, a Wasm binary can execute on virtually any platform, including web browsers and server-side environments. 
WebAssembly programs are compiled ahead-of-time from source languages such as C/C++, Rust, and Go, utilizing compilation pipelines like LLVM. 


\input{snippets/c_and_wasm}

% We do not enumerate the types, lets try to be as much agnostic to the version of Wasm as possible
WebAssembly programs operate on a virtual stack that allows primitive data types.
% These same data types are used to annotate instructions in the WebAssembly code.
Additionally, a WebAssembly program might include several custom sections.
For example, binary producers such as compilers use custom sections to store metadata, such as the name of the compiler that generates the Wasm code.
A WebAssembly program also declares memory sections and globals, which are used to store, manipulate and share data during program execution, e.g. to share data with the host engine of the WebAssembly binary.

WebAssembly is designed with isolation as a primary consideration. For instance, a WebAssembly binary cannot access the memory of other binaries or cannnot interact directly with browser's APIs, such as the DOM or the network. Instead, communication with these features is constrained to functions imported from the host engine, ensuring a secure and safe Wasm environment.
Moreover, control flow in WebAssembly is managed through explicit labels and well-defined blocks, which means that jumps in the program can only occur inside blocks, unlike regular assembly code \cite{10.1145/3062341.3062363}. 
\todo{Change the example to Rust}
In \autoref{example:cprogram}, we provide an example of a C program that contains a function declaration, a loop, a loop conditional, and a memory access. When the C code is compiled to WebAssembly, it produces the code shown in \autoref{example:wasmprogram}. The stack operations are folded with parentheses.
The module in the example contains the components described previously.

% Now we talk a bit about what is the state of a Wasm program.
%\todo{The following text should be revised. }

The WebAssembly runtime structure is described in the WebAssembly specification and it includes 10 key elements: the Store, Stack, Locals, Module Instances, Function Instances, Table Instances, Memory Instances, Global Instances, Export Instances, and Import Instances. These components interact during the execution of a WebAssembly program, collectively defining the state of a program during its runtime.

Two of these elements, the Stack and Memory instances, are particularly significant in maintaining the state of a WebAssembly program during its execution. The Stack holds both values and control frames, with control frames handling block instructions, loops, and function calls. Meanwhile, Memory Instances represent the linear memory of a WebAssembly program, consisting of a contiguous array of bytes.
In this paper, we highlight the aforementioned two components to define, compare and validate the state of two Wasm programs during their execution. 

\subsection{Semantic Equivalence}

Semantic equivalence refers to the notion that two programs or functions are considered equivalent if, for a given specified input domain, they produce the same output values or have the same observable behavior \cite{Semantic equivalence}. 
In other words, the semantics of the two programs are equivalent when the input-output relationship (w/ possibly some abstraction), even if the internal implementation details or the structure of the programs differ.


Let us illustrate this with an example.
Assume two programs $P$ and $P'$ (\autoref{example:state1} and \autoref{example:state2} respectively) where $P'$ is the result of modifying a code in the first instruction of its unique function.
The program $P'$ has two extra instructions right before returning from the function.
The remaining components of the original binary are not modified.


\input{snippets/state_example/state_mini}

The state of the program $P$ when entering the function is its stack $[S]$, 
the program $P'$ has the same state before executing the function.
The input values of the function for both programs are $L$, their outputs are the top of the stack at the end of the execution.


Program $P$ has the state $[[S:i32.const\ 1]]$ just before returning from the function execution.
When we trace the states of the program $P'$, we can construct the following sequence of states:
\begin{enumerate}
    \item $[[S: i32.const\ 1]]$ the integer constant 1 is now on the top of the stack.
    \item $[[S: i32.const\ 1, i32.const\ 42]]$ the integer constant 32 is  the top of the stack.
    \item $[[S: i32.const\ 1]]$ the top of the stack is dropped. The function execution stops.
\end{enumerate}
Notice that, the stack state of program $P'$ is the same as program $P$.
Thus, we can say that these two programs are semantically equivalent.
Even though the programs share semantic equivalence, they display differences during execution. 
Specifically, $P'$ stresses more on the stack by adding and subsequently dropping more values.
These subtle yet significant differences form the crux of the diversification approaches discussed in this study.






\subsection{Rewriting rule}
\label{rewriting}

Our definition of a rewriting rule draws from the one proposed by Sasnauskas et al. \cite{2017arXiv171104422S}, and integrates a predicate to specify the replacement condition.
Concretely, a rewriting rule is defined as a tuple, denoted as \texttt{(LHS, RHS, Cond)}. Here, \texttt{LHS} refers to the code segment slated for replacement, \texttt{RHS} is the proposed replacement, and \texttt{Cond} stipulates the conditions under which the replacement is acceptable.
Importantly, \texttt{LHS} and \texttt{RHS} are meant to be semantically equivalent, per the definition of previous section.


For example, the rewriting rule \texttt{(x,\ x\ i32.or\ x, \{\})} implies that the \texttt{LHS} 'x' is to be replaced by an idempotent bitwise \texttt{i32.or} operation with itself, absent any specific conditions.
Notice that, for this specific rule, the commutative property shared by \texttt{LHS} and \texttt{RHS}, symbolized as \texttt{(LHS, RHS) = (RHS, LHS)}.
Besides, the \texttt{Cond} element could be an arbitrary criterion. 
For instance, the condition for applying the aforementioned rewriting rule could be to ensure that the newly created binary file does not exceed a threshold binary size.

Based on our understanding, our research is the first to apply the concept of rewriting rules to WebAssembly.
This will expand the potential use cases of wasm-mutate. 
Beyond its role as a diversification tool, it can also be used as a standard tool for conducting program transformations in WebAssembly.


\section {Design of Wasm-Mutate}
\label{tech}
% Here is the state of wasm-mutate \url{https://github.com/bytecodealliance/wasm-tools/issues/415}.

In this section we present \tool, a novel technique to diversify
WebAssembly binaries and produce semantically equivalent variants.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/wasm-mutate-general.pdf}
    \caption{ \tool workflow and high level architecture.}
  \label{fig:wasm-mutate}
\end{figure*}

\subsection{Overview}
The primary objective of \tool is to perform WebAssembly to WebAssembly diversification, i.e., generate semantically equivalent variants from a given WebAssembly binary input. 
\tool's central approach involves synthesizing these variants by substituting parts of the original binary using rewrite rules. 
It leverages a comprehensive set of rewrite rules, boosted by a proposed random traversals of e-graph structures (refer to \autoref{alg}).


In \autoref{fig:wasm-mutate} we illustrate the workflow of \tool: it starts with a WebAssembly binary as its input (1).
It parses the original binary (2), turning the input program into tree abstractions, i.e., \tool builds the control flow graph and data flow graph. 
Using the defined rewriting rules, \tool builds an e-graph (3) for the original program.
Parts of the original program are randomly replaced by traversal of the e-graph (4).
The outcome of \tool is a semantically equivalent variant of the original binary (5).
The tool guarantees semantically equivalent variants because each individual rewrite rule is semantic preserving.
Notice that, the output result in step (5) can be connected again to step (1), enabling the stacking of multiple transforms with several iterations of \tool. 


\subsection{WebAssembly Rewriting rules}
%\todo{ideally one citation per transformation, we do not want to claim novelty on any of them, we claim novelty on the e-graph / integration part}

% \input{tables/rewriting_rules.tex}
%\pagebreak

%Each row is annotated with the guarantees of the transformation.
In total, there are 135 possible rewriting rules in \tool counting all possible applications of several meta-rules.
For example, 125 rewriting rules are implemented as part of a peephole meta-rule.
%\todo{that's a lot of rules. are they all implemented manually, or do we have some 'meta' rules, i.e. rules that actually can be applied in different ways}
We group all rewriting rules into 7 meta-rules that we present next.
These meta-rules are selected considering the difficulty of their implementation.

\textbf{Add type:}
In WebAssembly, the type section wraps definitions of signatures for the binary functions.
\tool implements two rewrite rules over this section, one of which is illustrated in the following rewriting rule. 

\input{snippets/structure_transform/add_type}

This transformation generates random function signatures with a random number of parameters and results count.
Notice that, this rewriting rule does not affect the runtime behavior of the  variant.
It also guarantees that the index of the already defined types is consistent after the addition of a new type. This is because Wasm programs cannot access or use a type definition during runtime, they are only used to validate the signature of a function during compilation and validation from the host engine.

From the security perspective, this transformation prevents against static binary analysis. 
For example, to avoid malware detection based on signature set \cite{CABRERAARTEAGA2023103296}.

\textbf{Add function:} The function and code sections of a Wasm binary contain the function  declarations and the code body of the declared functions, respectively.
\tool add new functions, through mutations in the two mentioned sections.
To add a new function, \tool selects a new random type.
Then, with the type signature, the random function body is created.
The body of the function consists of returning the default value of the result type.
The following example illustrates this rewriting rule.

\input{snippets/structure_transform/new_function}

\tool never adds a call instruction to this function.
So in practice, the new function is never executed.
Therefore, executing both, the original binary and the mutated one with the same input, lead to the same final state.
This strategy follows the work of Cohen, advocating the insertion of harmless "garbage" code into a program. 
These transformations do not impact the program's functionality; they increase its static complexity.

\textbf{Remove dead code:} \tool can randomly remove dead code.
In particular \tool removes: \emph{functions, types, custom sections, imports, tables, memories, globals, data segments and elements} that can be validated as dead code with guarantees.
For instance, to delete a memory declaration, the binary code must not contain a memory access operation. 
Separate mutators are included within \tool for each of the aforementioned elements.

For a more concrete example, the following listing illustrates the case of a function removal.

\input{snippets/structure_transform/remove_function}

When removing a function, \tool ensures that the resulting binary remains valid and semantically identical to the original binary: it checks  that the deleted function was neither called within the binary code nor exported in the binary external interface. 
As exemplified above, \tool might also eliminate a function import while removing the function. 

%\todo{unclear for the reader what an "import" is in Webassembly}

% There is one mention here, https://scholar.afit.edu/cgi/viewcontent.cgi?article=6059&context=etd, but it is a dissertation.
Eliminating dead code serves a dual purpose: it minimizes the attack surface available to potential malicious actors and strengthens the resilience of security protocols. 
For instance, it can obstruct signature-based identification \cite{CABRERAARTEAGA2023103296}.
Besides, Narayan and colleagues have demonstrated the feasibility of Return-Oriented Programming (ROP) attacks \cite{Swivel}.
Within the scope of ROP, the removal of dead code stops jumps to harmful behaviors within the binary. 
%\todo{I like the binary size, it's easy to understand, the only question is why to do it randomly and not systematically? TBD}
On the other hand, the act of removing dead code can be perceived as software debloating. 
This process reduces the binary's size, improving its overall quality. 
Moreover, debloating reinforces system security by removing potential vulnerabilities \cite{236200}.


\textbf{Edit custom sections:}
The custom section in WebAssembly is used to store metadata, such as the name of the compiler that produces the binary or the symbol information for debugging.
Custom sections are not part of the WebAssembly runtime structure.
Thus, this section does not affect the execution of the Wasm program.
\tool includes one mutator to edit custom sections. 
This is exemplified in the following rewriting rule. 

\input{snippets/structure_transform/custom_section_edit}

The \emph{Edit Custom Section} transformation operates by randomly modifying either the content or the name of the custom section. 
As illustrated by Cabrera-Arteaga et al. \cite{CABRERAARTEAGA2023103296}, such a rewriting strategy also acts as a potent deterrent against compiler identification techniques.
Furthermore, it can also be employed in an innovative manner to emulate the characteristics of a different compiler, \emph{masquerading} as another compilation source. 
This creates a dynamic scenario of a Moving Target Defense for static binary analysis. 
This strategy ultimately aids in shrinking the attack surface accessible to potential adversaries, hence enhancing overall system security.

 % you can be more precise and more original: compiler identitifcation. we may even talk about faking to be another compiler


\textbf{If swapping:} In WebAssembly, an if-construction consists of a consequence and an alternative. The branching condition is executed right before the \texttt{if} instruction; if the value at the top of the stack is greater than \texttt{0}, then the consequence-code is executed, otherwise the alternative-code is run.
The \emph{if swapping} rewriting swaps the consequence and alternative codes of an if-construction.
% This strategy has been employed in other software stacks as well.
% This strategy increases the difficulty for an attacker to predict the program's control-flow, making it more challenging to exploit vulnerabilities.

% https://ieeexplore.ieee.org/document/6915508
% Protecting JavaScript Apps from Code Analysis
% Composite Software Diversification

To swap an if-construction in WebAssembly, \tool inserts a negation of the value at the top of the stack right before the \texttt{if} instruction.
In the following rewriting rule we show how \tool performs this rewriting.
\input{snippets/code_motion/if_swap}
The consequence and alternative codes are annotated with the letters \texttt{A} and \texttt{B}, respectively.
The condition of the if-construction is denoted as \texttt{C}.
The negation of the condition is achieved by adding the \texttt{i32.eqz} instruction in the RHS part of the rewriting rule.
The \texttt{i32.eqz} instruction compares the top value of the stack with zero, pushing the value \texttt{1} if the comparison is true.
Some if-constructions may not have either a consequence or an alternative code.
In such cases, \tool replaces the missing code block with a single \texttt{nop} instruction.

Notice that, in the context of ROP \cite{Swivel}, this transformation can protect a victim binary to be exploited. 



\textbf{Loop Unrolling:} 
Loop unrolling is a technique employed to enhance the performance of programs by reducing loop control overhead and increasing the level of parallelism \cite{dongarra1979unrolling}. 
%Although the original functionality of the program remains unchanged, its static structure and performed operations are altered.
\tool incorporates a loop unrolling transformation and utilizes the Abstract Syntax Tree (AST) of the original Wasm binary to identify loop constructions. 

When \tool selects a loop for unrolling, its instructions are divided by first-order breaks, which are jumps to the loop's start. This separation ensures that branching instructions controlling the loop body do not require label index adjustments during unrolling. The same holds true for instructions continuing to the next loop iteration.
As the loop unrolling process unfolds, a new Wasm block is created to encompass both the duplicated loop body and the original loop. 
Within this newly established block, the previously separated groups of instructions are copied. 
These replicated groups of instructions mirror the original ones, except for branching instructions jumping outside the loop body, which need their jumping indices increased by one. This modification is required due to the introduction of a new \texttt{block ... end} scope around the loop body, which affects the scope levels of the branching instructions.

In the following text we illustrate the rewriting rule for a function that contains a loop. 
\input{snippets/code_motion/loop_unroll}

The loop in the LHS part features a single first-order break, indicating that its execution will cause the program to continue iterating through the loop. 
The loop body concludes right before the \texttt{end} instruction, which highlights the point at which the original loop breaks and resumes program execution.
Upon selecting the loop for unrolling, its instructions are divided into two groups, labeled \texttt{A} and \texttt{B}. 
As illustrated in the RHS part, the unrolling process entails creating two new Wasm blocks. 
The outer block encompasses both the original loop structure and the duplicated loop body, while the inner blocks, denoted as \texttt{A'} and \texttt{B'}, represent modifications of the jump instructions in groups \texttt{A} and \texttt{B}, respectively.
Notice that, any jump instructions within \texttt{A'} and \texttt{B'} that originally leaped outside the loop must have their jump indices incremented by one. 
This adjustment accounts for the new block scope introduced around the loop body during the unrolling process. 
Furthermore, an unconditional branch is placed at the end of the unrolled loop iteration's body. 
This ensures that if the loop body does not continue, the tool breaks out of the scope instead of proceeding to the non-unrolled loop.


Loop unrolling enhances resistance to static analysis while maintaining the original performance \cite{10.1145/3453483.3454035}. 
In particular, Crane et al. \cite{10.1145/2810103.2813682} validate the effectiveness of adding and modifying jumps instructions against Function-Reuse attacks.
Our rewriting rule has the same advantages, it unrolls loops while 1) incorporating new jumps and 2) editing existing jumps, as it can be observed with the addition of the \texttt{br_if}, \texttt{end}, and \texttt{br} instructions. 



\textbf{Peephole:} 
This transformation category is about rewriting instruction sequences within function bodies, signifying the most granular level of rewriting. 
We implement 125 rewriting rules for this group. 
The design and implementation of these rules is based on our previous work in CROW \cite{arteaga2020crow}, as well as on our experience in automatic transformation of \wasm.
In addition we include rewriting rules that affects the memory of the binary.
For example, we include rewriting rules that creates random assignments to newly created global variables.
For these rules, we incorporate several conditions, denoted by \texttt{Cond}, to ensure successful replacement. 
These conditions can be utilized interchangeably and combined to constrain transformations (see \autoref{alg}).

%\todo{reformulate: saya what the tool does, and then how it provides guarantees (by excluding)}
For instance, \tool is designed to confirm that instructions marked for replacement are deterministic. 
We specifically exclude instructions that could potentially cause undefined behavior, such as function calls, from being mutated. 
For this rewriting type, \tool only alters stack and memory operations, leaving the control frame labels unaffected.

The peephole category rewriting rules are meticulously designed and manually verified. 
%\todo{convoluted sentence, not crisp enough: The verification process is feasible because peephole transformations typically necessitate reviewing and replacing a concise sequence of instructions with more efficient alternatives.}
An instance of such streamlined transformation can is illustrated in \autoref{rewriting}, \texttt{(\ x\ i32.or\ x, x, \{\})} implies that the \texttt{LHS} 'x' is to be replaced by an idempotent bitwise \texttt{i32.or} operation with itself, in the absence of any specific conditions.
Therefore, this category continues to uphold the benefits previously discussed under the \emph{Remove Dead Code} category.

%\todo{J: Maybe add here somekind of conclusion about how all RR aim for: security, obfuscation, testing and anti-debugging. WDYT?}

\subsection{E-graphs in Wasm-Mutate}
We build \tool on top of e-graphs.
% Introduce concepts of node, eclass
An e-graph is a graph data structure utilized for representing rewriting rules \cite{10.1145/3571207}. 
In an e-graph, there are two types of nodes: e-nodes and e-classes. 
An e-node represents either an operator or an operand involved in the rewriting rule, while an e-class denotes the equivalence classes among e-nodes by grouping them, i.e., an e-class is a virtual node compound of a collection of e-nodes. 
Thus, e-classes contain at least one e-node.
Edges within the graph establish operator-operand equivalence relations between e-nodes and e-classes.

An e-graph is automatically built from a program by analyzing its expressions and operations through its data flow graph.
Then, each unique expression, operator, and operand are transformed into e-nodes.
Based on the input rewriting rules, the equivalent expressions are detected, grouping equivalent e-nodes into e-classes.
During the detection of equivalent expressions, new operators could be added to the graph as e-nodes.
Finally, e-nodes within an e-class are connected with edges to represent their equivalence relationships.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/egraph1.pdf}
    \caption{e-graph for idempotent bitwise-or rewriting rule. Solid lines represent operand-operator relations, and dashed lines represent equivalent class inclusion. }
  \label{e-graph}
\end{figure}

% How to construct
For example, let us consider one program with a single instruction that returns an integer constant, \texttt{i64.const 0}. Let us also assume a single rewriting rule, \texttt{(x,\ x\ i64.or\ x, x instanceof i64)}. 
In this example, the program's control flow graph contains just one node, representing the unique instruction.
The rewriting rule represents the equivalence for performing an \texttt{or} operation with two equal operands.
\autoref{e-graph} displays the final e-graph data structure constructed out of this single program and rewriting rule. 
We start by adding the unique program instruction \texttt{i64.const 0} as an en e-node (depicted by the leftmost solid rectangle node in the figure). 
Next, we generate e-nodes from the rewriting rule (the rightmost solid rectangle) by introducing a new e-node, \texttt{i64.or}, and creating edges to the \texttt{x} e-node.
Following this, we establish equivalence. 
The rewriting rule combines the two e-nodes into a single e-class (indicated by the dashed rectangle node in the figure). 
As a result, we update the edges to point to the \texttt{x} symbol e-class.

%\todo{explain what "equality saturation" means in this context. do we use equality saturation for diversification? I'm not sure}
%Since there are no more rewriting rules to apply, we reach equality saturation, signifying that the graph can no longer change. Therefore, we have successfully constructed the e-graph.

\paragraph{e-graph Traversal Algorithm}
\label{alg}
% General use of case
While e-graphs possess numerous significant properties, we focus on what we believe is the most crucial for the purpose of diversification. We articulate this key attribute as an invariant in the text that follows.

\begin{tcolorbox}[boxrule=1pt,arc=.3em,boxsep=-1.3mm]
Any path traversal through the e-graph results in a semantically equivalent expression
\end{tcolorbox}

%\todo{which invariant? why does it matter?}
The invariant is exemplified in \autoref{e-graph}, where an infinite concatenation of "or" operations can be constructed. 
In this work, we take advantage of this invariant to generate mutated variants of an original program. 
The e-graph can be traversed randomly, selecting an e-node within each e-class that we visit, producing a random-but-equivalent expression. 
%We can extract an infinite number of semantically equivalent expressions from the e-graph, as shown in previous research \cite{10.1145/3571207, 10.1145/3434304}.

%It selects a random instruction within the binary functions and applies one or more of the 125 rewrite rules.
%Finally, it re-encodes the WebAssembly module with the new, rewritten expression to produce a binary variant. 


% - SMT and egraphs are complete different things. Therefore, we need to evaluate the soundness of the egraphs generated programs modulo Input => CROW
 
% - wasm-mutate only applies egraphs if the CFG of the peephole is complete/deterministic.


\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\begin{algorithm}
    \footnotesize
	\begin{algorithmic}[1]
	%	\Procedure{MyProcedure}{}
        \Procedure{traverse}{$egraph$, $eclass$, $depth$}
        % Input:
        %\Comment{Input: e-graph, max-depth}
        % Output:
        %\Comment{Output:y}
        \If{depth = 0}
          \State  \Return \textbf{smallest\_tree\_from}(egraph,\ eclass)
        \Else
            \State $nodes \gets egraph[eclass]$
            \State $node \gets random\_choice(nodes)$
            \State $expr \gets (node, operands=[])$
            \ForEach {$child \in node.children $}
                \State $subexpr \gets \textbf{TRAVERSE}(egraph,\ child,\ depth - 1)$
                \State $expr.operands \gets expr.operands \cup\ \{subexpr\}$
            \EndFor
            \State \Return $expr$
        \EndIf
        \EndProcedure
	\end{algorithmic} 
	\caption{e-graph traversal algorithm.} 
	\label{peephole:mutator}
\end{algorithm}

%\todo{can we make a strong claim here? does everybody maximize simplification with equality saturation? are we the first to do random path in e-grahs?}
We propose an algorithm to randomly traverse an e-graph and generate  semantically equivalent program variants.
The e-graph traversal is summarized in Algorithm \autoref{peephole:mutator}. 
To the best of our knowledge, \tool, is the first practical implementation of the random e-graph traversal algorithm.
It receives an e-graph, an e-class node (initially the root's e-class), and the maximum depth of expression to extract. The depth parameter ensures that the algorithm is not stuck in an infinite recursion. We select a random e-node from the e-class (lines 5 and 6), and the process recursively continues with the children of the selected e-node (line 8) with a decreasing depth. As soon as the depth becomes zero, the algorithm returns the smallest expression out of the current e-class (line 3). The subexpressions are composed together (line 10) for each child, and then the entire expression is returned (line 11). 

\paragraph{Example of a random e-graph traversal}
Let's demonstrate how the proposed traversal algorithm can generate program variants with an example. 
We will illustrate Algorithm \ref{peephole:mutator} using a maximum depth of 1. 
\autoref{example:peeporig} presents a hypothetical original Wasm binary to mutate. 
In this example, the developer has established two rewriting rules: \texttt{(x, x i32.or x, x instanceof i32)} and \texttt{(x, x i32.add 0, x instanceof i32)}. The first rewriting rule represents the equivalence of performing an \texttt{or} operation with two equal operands, while the second rule signifies the equivalence of adding 0 to any numeric value.
By employing the code and the rewriting rules, we can construct the e-graph depicted in \autoref{e-graph3}. The figure demonstrates the operator-operand relationship using arrows between the corresponding nodes.


\input{snippets/peephole/peep_example.tex}


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/egraph3.pdf}
    \caption{e-graph built starting in the first instruction of \autoref{example:peeporig}. }
  \label{e-graph3}
\end{figure}


In \autoref{e-graph3}, we annotate the various steps of Algorithm \ref{peephole:mutator} 
for the scenario  described above. Algorithm \ref{peephole:mutator} begins at the e-class containing the single instruction \texttt{i64.const 1} from \autoref{example:peeporig}. 
It then selects an equivalent node in the e-class (2), in this case, the \texttt{i64.or} node, resulting in:
{\texttt{expr = i64.or l r}}.
The traversal proceeds with the left operand of the selected node (3), choosing the \texttt{i64.add} node within the e-class: 
{\texttt{expr = i64.or (i64.add l r)} \texttt{r}}.
The left operand of the \texttt{i64.add} node is the original node (5): 
{\texttt{expr = i64.or (i64.add i64.const 1 r)} \texttt{r}}.
The right operand of the \texttt{i64.add} node belongs to another e-class, where the node \texttt{i64.const 0} is selected (6)(7):
{\texttt{expr = i64.or (i64.add i64.const 1 i64.const 0)} \texttt{r}}.
In the final step (8), the right operand of the \texttt{i64.or} is selected, corresponding to the initial instruction e-node, returning:
{\texttt{expr = i64.or (i64.add i64.const 1 i64.const 0)\ i64.const 1}}
The traversal result applied to the original Wasm code can observed in \autoref{example:peepapplied}. 

\subsection{Implementation}

\tool is implemented in Rust, comprising approximately 10767 lines of Rust code. 
We leverage the capabilities of the wasm-tools project of the bytecodealliance for parsing and transforming WebAssembly binary code. 
Specifically, we utilize the wasmparser and wasm-encoder modules for parsing and encoding Wasm binaries, respectively.
The combination of wasmparser and wasm-encoder enables \tool to rewrite Wasm binaries. 
%All operations are done lazily: only the necessary sections of the binary are parsed and rewritten during the transformation of the input Wasm by \tool.
The implementation of \tool can be found at \url{https://github.com/bytecodealliance/wasm-tools}.

%\todo{J: Should we talk about determinism in the transformations?}



%Although the construction of the e-graph itself can be expensive, we can reuse the e-graph many times after construction, amortizing the construction costs. 
%We can also speed up construction by limiting the number of rewrites applied to the e-graph. 


%\textbf{Add global}
%\todo{Move this to special operators in the peephole mutator}
%\input{snippets/structure_transform/new_global.tex}

%\subsection{Properties}

%- Ergodicity (validates malware evasion paper)

%- Faster (compare to other rewriting tools, CROW)

%- Taxonomy, easy to measure and control the rewriting space. Define properties and metrics, e.g. size of the rewriting.

\section {Evaluation}
\label{eval}

In this section, we outline our methodology for evaluating \tool.
Initially, we introduce our research questions and the corpus of programs that we utilize for the generation and assessment of \tool while preventing Spectre attacks.
Subsequently, we elaborate on the methodology for each research question.

% \subsection{Research Questions}

\newcommand\rqstatic{To what extent are the program variants generated by \tool statically different at the WebAssembly and Machine Code levels?\xspace}

\newcommand\rqdynamic{ To what extent are the program variants generated by \tool dynamically different?\xspace}

\newcommand\rqdefensive{To what extent does \tool prevent Spectre side-channel attacks on \Wasm programs?\xspace}


\newcommand\rqperformance{To what extent does \tool affects \Wasm variants performance?\xspace}

\newcommand\rqofensive{To what extent are the program variants generated by \tool harmful?\xspace}


\newcommand\rqtesting{To what extent can \tool be used to perform differential testing of \Wasm tools?\xspace}

\newcommand{\nProgramsRosetta}{303\xspace}


\newcommand{\DTWStatic}{\ensuremath{\mathit{dt\_static}\xspace}}
\newcommand{\DTWDynamic}{\ensuremath{\mathit{dt\_dy}\xspace}}

\begin{enumerate}[label=RQ\arabic*:, ref=RQ\arabic*]
    % From CROW %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
     \item \label{rq:static} \textbf{\rqstatic}
        We  check whether the \wasm binary variants produced by \tool are different from the original \wasm binary. %Then, we assess whether the generation of x86 machine code performed by  wasmtime engine preserves \tool's transformations.
    
    \item \label{rq:dynamic}\textbf{\rqdynamic} 
        It is known that not all diversified programs produce distinguishable executions \cite{crane2015thwarting}. 
        We check for the presence of different behaviors by collecting the execution traces out of wasmtime, characterizing the behavior of a \wasm program by its memory and instruction traces.
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        
    \item \label{rq:defensive}\textbf{\rqdefensive} 
     \label{rq:performance} Diversification has been proved to prevent security issues. We asses the impact of \tool in preventing Spectre attacks for \Wasm.
    
    %\item \label{rq:ofensive}\textbf{\rqofensive} 
    
\end{enumerate}

\subsection{Corpora}
\label{sec:corpus}

\input{tables/dataset}

We answer our research questions with a corpus of 303 + 4 programs.
These programs are summarized in \autoref{tab:corpus}.
Each row in the table corresponds to programs, with the columns providing the following information: Source of the Program wich identifies where the program was sourced from, Program Count or Name which specifies the number of programs, if multiple programs are from the same source, or the name of the program if the source contains only one, Research Question Addressed, Function Count which provides the total number of functions present in each original \wasm program. Instruction Count which indicates the total number of instructions found in the original \wasm programs, Attack Performed which references the type of attack that the original program was subjected to and Building Process which describes the process employed to convert the source code into the corresponding \wasm programs.

We answer \ref{rq:static} and \ref{rq:dynamic} with the same corpus of programs proposed by Cabrera \etal \cite{arteaga2020crow}, as it is shown in the first row of \autoref{tab:corpus}.
The corpus contains \nProgramsRosetta from Rosetta code, curated in early 2020.
The corpus contains programs in a range of tasks, from simple ones, such as adding two numbers, to complex algorithms like a compiler lexer. 
The number of functions for each program ranges from 7 to 103 and, the number of total instructions ranges from 170 to 36023.
All programs in corpus: 1) do not require input from user, \ie invoking functions like \texttt{scanf}, 2) terminate, 3) are deterministic, \ie given the same input, provide the same output and 4) generates valid \wasm programs when using \texttt{wasi-clang}.

We answer \ref{rq:defensive} with four \Wasm programs and three Spectre attack scenarios, as presented in Swivel \cite{Swivel}. 
These programs are summarized in the final four rows of our corpus table.
The first two programs, manually crafted for simplicity, each contain 16 functions, with instruction counts of 743 and 297, respectively. These binaries are specifically designed to facilitate the execution of the Spectre branch target attack.
The third and fourth programs, documented in rows four and five, correspond to attacks ported from the Safeside project \cite{safeside}. 
Unlike the first two, these binaries are significantly larger, each containing nearly 3000 functions and more than 300000 instructions. 
They are utilized for conducting the Spectre Return Stack (RSB) and Spectre Pattern History (PHT) attacks.
Notice the difference in the number of functions and instructions between the first pair of Swivel binaries and the latter pair. 
This disparity can be attributed to the varying compilation processes applied to these \Wasm binaries. 
The first two were manually tailored to streamline the attack processes, whereas the other two programs were directly compiled from Safeside.
The three attack scenarios are described in details in \autoref{protocol:rq3}.

%% FROM CROW RQ1
\subsection{Protocol for RQ1}
\label{protocol:rq1}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/protocol.pdf}
    \caption{Protocol to answer \ref{rq:static} and \ref{rq:dynamic}}
  \label{protocol}
\end{figure}


With \ref{rq:static},
we assess the ability of \tool to generate \Wasm binaries that are different from the original program.
In \autoref{protocol} we show the the steps we follow to answer \ref{rq:static}.
We run \tool on our corpus of \nProgramsRosetta{} original C programs (step (1) in figure). 
To generate the variants:
% Select wih replacement algo
1) we start with one original and pass it to \tool to generate a variant;  
2) the variant and the original program form a population of programs; 
3) we randomly select a program from this population and pass it to \tool to generate a variant, which we add to the population; 
4) we then restart the process in step 3). 
This procedure is carried out for a duration of 1 hour.
The final outcome (step (2) in figure) is a population with randomly stacked transformations all starting from an original \wasm program.
We determine the number of unique variants in the population.
We compute the sha256 hash of each variant bytestream in order and define the population size metric as: 


\todo{Add legend X to all figures.}
\todo{Replace wasmtime JIT compiler by cranelift along with the text and the protocol figure.}

\begin{metric}{Population size:}\label{metric:pop}
Given an original \wasm program P a generated corpus of \wasm programs $V=\{v_1, v_2, ..., v_N\}$ where $v_i$ is a semantically equivalent variant of P, the population size is defined as:
$$
    | set(\{ sha256(v_1), ... sha256(v_N) \})|\text{ }\forall v_i \in V 
$$
\end{metric}


Since \wasm binaries are further transformed into machine code before they execute by wasmtime, we also check that this additional transformation preserves the difference introduces by \tool in the \wasm binary. 
We use the wasmtime JIT compiler, cranelift, with all their possible optimizations, to generate the x86 binaries for each \wasm program and its variants  (step (3) in figure). 
Then, we calculate the number of unique variants machine code representation for wasmtime.
Counting the number of unique machine code, we asses for the diversification preservation of the generated variants: \\

\begin{metric}{Ratio of preserved variants:}\label{def:preservation}
    Given two \wasm variants from the same original program $P_1$ and $P_2$ and $C$ a JIT compiler:
    if $hsh(P_1)$ $\neq$ $hsh(P_2)$ and $hsh(C(P_1))$) $\neq$ $hsh(C(P_2))$, this means that  both programs are still different when compiled to machine code, and we conclude that the cranelift compiler $C$ does not remove the transformations made by \tool.  
    
\end{metric}

Notice that, \autoref{def:preservation} is applicable only when comparing variants of the same program, the original included. 
%The process of measuring preservation serves as a method to evaluate how compilers handle semantically equivalent \wasm variants and to determine a diversifier's quality at a level that higher than machine code. 
%Furthermore, this allows us to assess the efficiency of the compiler's optimization passes. 
%We chose wasmtime due to that it is widely recognized as the most reliable and extensively used \wasm engine for backend tasks. 




%% FROM CROW RQ2
\subsection{Protocol for RQ2}

For \ref{rq:dynamic}, we compare  the executions of a program and its variants for a given input. 
This process can be appreciated in the enclosed square of \autoref{protocol}, annotated with \ref{rq:dynamic}.
We execute the program using a wasmtime embedded host.
In this experiment, we characterize the  execution of a \wasm binary according to its traces and execution times.

We first collect \wasm binaries for which wasmtime generates unique machine machine code, i.e., variants that are preserved (refer to \autoref{protocol:rq1}).
Then we execute each preserved \wasm variant for each original program.
For each execution of a \wasm program, we gather the instructions and memory traces utilizing IntelPIN with our own pintool implementation \cite{luk2005pin, 10.1145/3478520} (step (4) in the figure).
To only collect the traces of the Wasm execution with a wasmtime engine, we pause and resume the collection as the execution leaves and re-enters the Wasm code, respectively.
We implement this filtering with the built-in hooks of wasmtime.
In addition, we disable ASLR on the machine where the variants are executed.
This latter action ensures that the placement of the instructions in memory is deterministic.
For the memory we take those events that originate from an instruction of a Wasm code and end up into the linear memory region of the same Wasm binary.
Examples of the traces we collect can be seen in \autoref{example:trace1} and \autoref{example:trace2} for memory and instruction traces, respectively.

\input{snippets/metrics/trace_mem}

\begin{metric}{Ratio of different traces:}\label{metric:mem:sha}
Given an original \wasm program P a generated corpus of \wasm programs $V=\{v_1, v_2, ..., v_N\}$ where $v_i$ is a semantically equivalent variant of P and a collection of traces of type $E$, $T=\{P_E, v_1E, v_2E, ...v_3E\}$ out of executing each variant, the ratio of different traces of an event type E is defined as:
$$
    \frac{| set(\{ sha256(v_1E),..., sha256(v_NE) \})|}{| \{ sha256(v_1E),..., sha256(v_NE) \}|}\text{ } \forall v_i \in T
$$

\end{metric}

Notice that the previously defined metric is instantiated for instructions and memory type of events.
A Ratio of 1 means that all traces are different, a ratio of 0 represents a collection of variants that all produce exactly the same traces.

We also calculate the execution times for the preserved variants and their corresponding original programs.
This can be appreciated in step (5) of the figure.
For each compiled population we randomly select 100 binaries including the original.
We then execute each binary 100 times to collect a distribution of 100 execution times.


\subsection{Protocol for RQ3}
\label{protocol:rq3}

\newcommand{\poct}{\emph{Cache timing POC}\xspace}
\newcommand{\pocd}{\emph{Differential computing POC}\xspace}
\newcommand{\pocp}{\emph{Port contention POC}\xspace}

To answer \ref{rq:defensive}, we apply \tool to the same security Proof of Concepts(POC) 
as the ones used by Narayan et al. to evaluated Swivel's ability at protecting \wasm programs \cite{Swivel}. 
The four cache timing side-channel attacks are presented in detail in \autoref{sec:corpus}. 
We evaluate to what extent \tool can prevent such attacks.
In the following text, we describe the POCs used to answer \ref{rq:defensive}.

%\todo{move the description of the four binaries in  \autoref{sec:corpus} and here add a short description of the intuition why \tool could be useful to prevent the attacks}

% Describing the Speculative execution  POC
Narayan and colleagues successfully navigated around the control flow integrity safeguards outlined in \cite{Swivel} for \Wasm, using speculative code execution as detailed in \cite{Spectre}. 
Thus, we use the same three Spectre attacks evaluated in Swivel.
Which specific binary and its corresponding attack can be appreciated in \autoref{tab:corpus}.
First, Spectre Branch Target Buffer (btb) attack. 
This attack exploits the branch target buffer by predicting the target of an indirect jump, thereby rerouting speculative control flow to an arbitrary target, similar to a ROP-style attack \cite{?}.
Second, Spectre Pattern History Table (pht).
It takes advantage of the pattern history table to anticipate the direction of a conditional branch during the ongoing evaluation of a condition. 
Third, Spectre Return Stack Buffer (ret2spec) attack. 
This attack exploits the return stack buffer that stores the locations of recently executed call instructions to predict the target of \texttt{ret} instructions. 
The attack methodology hinges on the extraction of memory bytes from another hosted \wasm binary that executes in parallel.

%\todo{TBD: in the end, I do not think we need this, we are not comparing their approach.}
%Swivel is designed to thwart attacks \cite{Swivel} that rely on three specific low-level instructions supplied by any standard libc implementation: the rdtsc instruction for measuring execution time, the clflush instruction for evicting a particular cache line, and the mfence instruction which allows pending memory operations to complete. 
%These instructions are not commonly accessible to Wasm through WASI implementations.
%Following the assumptions of the initial attack, we have made these instructions available within an embedded wasmtime host implementation. 
%This adaptation simplifies the POC cases.

For each of the four \wasm binaries introduced in \autoref{sec:corpus}, we generated a maximum of 1000 random transformations utilizing 100 distinct seeds. 
This resulted in a total of 100,000 variants for each original \wasm binary.
We then assessed the success rate of attacks across these variants by measuring the bandwidth of the exfiltrated data, that is: the rate of correctly leaked bytes per unit of time. 
To do so, we execute each variant the same setup of Swivel without mitigations.
The bandwidth metric, proposed by Google's Safeside project \cite{safeside}, serves to quantify the effectiveness of countermeasures.
To determine the bandwidth, we executed each variant and examined the exfiltrated data, represented as a sequence of bytes. 
We counted the correctly exfiltrated bytes and divided them by the variant program's execution time. 
It's important to note that this metric captures not only the binary determination of whether the attacks are successful or not, but also the degree to which the data exfiltration is hindered.
For instance, a variant that continues to exfiltrate secret data but does so over an impractical duration would be deemed as having been hardened. 
This approach recognizes that reducing the efficiency of an attack can be nearly as valuable as stopping it.
In the following definition we state the bandwidth metric:

\begin{metric}{Bandwidth:}\label{metric:ber}
Given data $D=\{b_0, b1, ..., b_C\}$ being exfiltrated in time $T$ and $K = {k_1, k_2, ..., k_N}$ the collection of correct data bytes, the bandwidth metric is defined as:
$$
    \frac{|b_i\text{ such that } b_i \in K|}{T}
$$
\end{metric}






%T%o answer \ref{rq:defensive} we compute \autoref{metric:ber} for each original POC program and its variants generated with \tool.

% Describing the port contention  POC
 

%\subsection{Protocol for RQ4}


%\subsection{Traces}




\section{Experimental Results}
\label{results}

\subsection{\rqstatic}
\label{rq:static:results}

\newcommand{\preserved}{72\%\xspace}

% Recap
To address \ref{rq:static}, we utilize \tool to process the original 303 programs proposed in CROW\cite{arteaga2020crow}. 
\tool is set to generate variants with a timeout of one hour for each individual program. 
Following this, we assess the sizes of their variant populations as well as their corresponding compiled population sizes. 
This process facilitates the evaluation of diversification preservation (Refer to \autoref{metric:pop} and \autoref{metric:popcomp} for more details).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/rq1/population.pdf}
    \caption{Number of unique \wasm programs generated by \tool in 1 hour out of each program of the CROW's corpus.}
  \label{rq1:plot:population}
\end{figure}


% Explain figure 4 and how wm outperform CROW in number of variants
In \autoref{rq1:plot:population}, we show the population size generated out of \tool. 
Each X point corresponds to a particular program, arranged in descending order based on population size. 
The Y-axis denotes the population size, expressed on a logarithmic scale. 

% General numbers
\tool successfully diversifies all 303 original programs, yielding a diversification rate of 100\%. 
Within an hour, \tool demonstrates its impressive efficiency and effectiveness by producing a median of 9500 unique variants for the 303 original programs.
There are several factors contributing to large population sizes. 

% More space to diversify
Significantly, \tool showcases the unparalleled capability to diversify functions within WASI-libc. 
A multitude of functions within the diversified binaries are incorporated by the C to \wasm compiler as components of the standard library. 
Despite the relatively low function count in the original source code, the transformative prowess of \tool enables it to create thousands of distinct variant populations. 
This feature sets it apart from, and marks an impressive advancement over, methods that can only diversify the original source code processed through the LLVM compilation pipeline. 
Furthermore, the employment of an e-graph to model semantically equivalent codes significantly enhances variant generation. 
As previously highlighted, an e-graph unlocks the possibility of generating an infinite number of variants, thereby creating a much larger space for diversity and increased resilience against potential threats.


% Now why not all populations are not the same
We have observed a significant variation in the population size out of \tool between different programs, ranging by several thousand variants. 
For example, the largest population size observed is 53816, while the smallest is 5716.
This disparity is primarily attributed to the intrinsic workings of \tool. 
It parses the input binary, then attempts to mutate a randomly selected portion of it. 
If the selected instruction is determined to be non-deterministic, despite the transformation being semantically equivalent, \tool discards the variant and moves on to another random transformation.
For instance, if the instruction targeted for mutation is a function call, \tool proceeds to the next one.
This process, in conjunction with the unique characteristics of each program, results in a varying population size. 
For example, an input binary with a high number of function calls would lead to a greater number of trials and errors, slowing down the generation of variants, thereby resulting in a smaller overall population size for 1 hour of \tool execution.

% Describing the preservation plot
As outlined in \autoref{protocol:rq1}, we also account for \autoref{def:preservation} by measuring the size of the compiled population. 
In \autoref{rq1:plot:preservation}, we show \autoref{metric:popcomp} for the wasmtime JIT compiler.
In this plot, each bar corresponds to a program's population size divided by the \wasm population size.
The order of the bars follows the same order from the \wasm population sizes depicted in \autoref{rq1:plot:population}.
%, arranged in descending order. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/rq1/preservation.pdf}
    \caption{Ratio of preserved variants.}
  \label{rq1:plot:preservation}
\end{figure}

% General numbers
In the case of cranelift, we observed that the compiled population size accounts for about \preserved of the original population size, suggesting a preservation rate of \preserved. 
The reason why we don't see a full 100\% preservation rate lies in \tool's capacity to mutate structures in the original \wasm binary that compilers typically ignore. 
For instance, custom sections, are disregarded by compilers, yet, they contribute to the variability of the \wasm population.
Similarly, bloated code is another factor. 
\tool generates certain variants that contain artificial and unused types or functions, which are promptly detected and eliminated by compilers.

Interestingly, we've observed another noteworthy case.
While compilers can easily remove superfluous code inserted by \tool, such as \texttt{nop} instructions, they may be constrained by the quantity of resources they can allocate for optimizing machine code generation. 
Consequently, some transformations, albeit "easy" to detect, might exceed this resources, and end up being retained in the compiled machine code.

%On the other hand, while the machine code generated by V8 varies more than the ones generated with wasmtime, the underlying structure remains consistent. 
%V8 employs random register allocation during code generation, which often results in machine code that utilizes the same instructions but alters the operands. 
%This was a discusssion that I had in Dagsthul, people are concerning about how the cranelift compilers is extremely deterministic.
%In contrast, wasmtime follows a deterministic approach in its code generation.


% While preservation is not necesarilly good...conclude with this
While a lower preservation rate might not appear advantageous at first glance, our observations highlight that unpreserved variants can put certain compiler components under stress, components that remain dormant during the execution of the original binary. 
In essence, specific optimization implementations may remain idle if the corresponding code isn't included in the \wasm program set for compilation. 
Importantly, despite considering the smallest population size coupled with the lowest preservation percentage, the range of machine codes executed still encompasses thousands of variants, underscoring the effectiveness of diversification strategies.


%\begin{figure*}
%    \centering
%    \includegraphics[width=0.9\linewidth]{plots/rq1/count.pdf}
%    \caption{TODO.}
%  \label{programs_count}
%\end{figure*}




\begin{tcolorbox}[boxrule=1pt,arc=.3em,boxsep=-1.3mm]
  \textbf{Answer to \ref{rq:static}}: \tool generates \wasm variants for the 303 programs, which are different from the original. 
  Within a one-hour diversification budget, \tool synthesizes more than 9000 unique  variants for each program. 
  The wasmtime JIT compiler preserves \preserved of the variants, and the number of unique machine code variants still measures in the thousands, underscoring the impressive diversification capability of \tool.
  The variants generated by \tool can be used to compare and measure the impact of different compiler optimizations.
\end{tcolorbox}


\subsection{\rqdynamic}

To answer \ref{rq:dynamic} we execute each preserved variant and its corresponding original program with wasmtime.
We execute those variants that are preserved in the machine code generated out of wasmtime.
We then collect its execution times and, instructions and memory traces with IntelPIN.
Finally, for each compiled population, we calculate the number of different traces (refer to \autoref{metric:mem:sha}).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/rq2/memory.pdf}
    \caption{Ratio of different memory traces. The higher the better.}
  \label{rq2:plot:mem}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/rq2/instructions.pdf}
    \caption{Ratio of different instruction traces. The higher the better.}
  \label{rq2:plot:ins}
\end{figure}

% Plot 1 and 2 for different traces



In \autoref{rq2:plot:mem} and \autoref{rq2:plot:ins} we plot the uniqueness ratio for memory and instructions respectively.
Each vertical opaque line represents an original program.
The blue dots represent the uniqueness ratio from 0 to 1.
For each compiled population we have observed that there is at least on variant that offer different execution traces, for both memory and machine code instructions.
The median number of different traces for memory is 0.35 and 0.19 for memory and machine code instructions respectively.

% Source of different mem traces
We have identified four primary factors responsible for the divergence in execution traces among \tool generated variants.
First, modifications to the binary layout—even those that don't affect executed code—inevitably alter both instruction traces and memory accesses within the program's stack. 
Specifically, \tool generates variants that modify the return addresses of functions, which consequently leads to differences in execution flow and memory accesses.
Second, one of our rewriting rules incorporates artificial global values into \wasm binaries. 
Since these global variables are inherently manipulated via the stack, and given that the stack is located within linear memory, their access inevitably generates divergent memory traces.
Third, \tool injects 'phantom' instructions which don't aim to modify the outcome of a transformed function during execution. 
These intermediate calculations trigger the spill/reload component of the wasmtime compiler, varying spill and reload operations. 
In the context of limited physical resources, these operations temporarily store values in memory for later retrieval and use, thus creating unique memory traces.
Finally, certain rewriting rules implemented by \tool replicate fragments of code, e.g., performing commutative operations. 
These code segments may contain memory accesses, and while neither the memory addresses nor their values change, the frequency of these operations does.
Overall, these findings illuminate the key influences shaping the diversity of execution traces among the executed variants. 
Gaining insights into these factors is indispensable for the evolution of more effective and efficient diversification strategies.




%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{images/shifted_instructions.png}
%\end{figure}



% Different instruction count and memory count
Moreover, we have observed considerable variation in the count of instructions, inclusive of memory accesses, among the distinct variants generated by \tool.
Intriguingly, a significant portion of these variants have a lower instruction count than their original program. 
This implies that \tool has the capability to enhance optimization of the original program, surpassing the innate optimizations employed by the wasmtime compiler. 
As a result, \tool is able to produce variants that outperform the original program in terms of execution time.
To provide a clearer depiction of this occurrence, we offer a graphical representation of the median execution times for all variants along with their original programs in \autoref{rq2:plot:execution_times}. 
Each individual data point on the plot signifies the median execution time for a specific variant, whereas the orange markers represent the median execution time for the original program.
Moreover, each vertical bar within the plot designates a distinct program population. 
Notice that relative comparisons between different vertical groups do not provide meaningful insights, as each group represents a distinct population of variants.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/rq2/times.pdf}
    \caption{Execution times.}
  \label{rq2:plot:execution_times}
\end{figure}

% Words on the workload of each program
Notice that, each program and its variants are executed with their initial input.
In practice, the traces we collect are generated from instantiating the \wasm binary and then executing its main function. 
For most programs in the CROW suite, their main function carries out a specific algorithm (for instance, \emph{base64 encoding}) using a simple, single-input model. 
This approach helps to collect execution traces without crafting programs input.
Yet, it potentially reduce the uniqueness of the traces collected. 
For instance, sections of the program subject to conditional execution might not be activated, not necessarily due to their 'dead-code' status, but rather due to the limited variety of inputs. 
However, even with this relatively modest input-testing approach, \tool still succeeds in providing unique execution traces for each program and its variants.
It's worth noting that the ratio of unique traces would likely increase given a broader input spectrum.

% Conclude with a history of X original, generates in median 9000 variants in 1 hour, after preservation in wasmtime this number goes to 4000, after our numbers...1000 offer different execution traces.
Reflecting on the findings from \ref{rq:static} regarding preservation, we notice that the number of generated variants displaying diverse runtime behaviors remains remarkable. 
Let us illustrate this with an example.
\tool generates a median of 9000 variants for the study subjects derived from CROW in only 1 hour. 
Of these \wasm variants, around \preserved are preserved in wasmtime, resulting into an effective total of 5040 distinct machine code programs.
Considering our data in \ref{rq:dynamic}, approximately 2000 (0.35$\times$5040) of these programs present unique execution traces, either through differentiated memory accesses or the overall variation in executed instructions. 
Therefore, \tool effectively generates \wasm program variants that challenge all aspects of a \wasm binary's lifecycle within wasmtime: from the initial compilation into machine code to its ultimate execution. 
This showcases the profound influence of \tool on diversifying and stressing the runtime environment of \wasm binaries.


\begin{tcolorbox}[boxrule=1pt,arc=.3em,boxsep=-1.3mm]
  \textbf{Answer to \ref{rq:dynamic}}:   \tool crafts variants that showcase diverse instruction traces, inclusive of distinct memory accesses. 
  While it's true that not every preserved variant yields a unique execution trace, in every case, there's at least one variant among the compiled population that does. 
  Beginning with the generation of thousands of \wasm variants, \tool guarantees unique execution traces for a substantial portion of them. 
  This holds true despite factors such as non-preservation and the generation of dead code. 
\end{tcolorbox}

\subsection{\rqdefensive}



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{plots/rq3/results.rq3.pdf}
    \caption{Visual representation of \tool's impact on Swivel's original programs. The Y-axis denotes exfiltration bandwidth, with the original binary's bandwidth under attack highlighted by a blue marker and dashed line. Variants are clustered in groups of 100 stacked transformations, denoted by green dots (median bandwidth) and lines (interquartile bandwidth range). Overall, for all 100000 variants generated out of each original program, 70\% have less data leakage bandwidth.}
  \label{attacks:impact}
\end{figure*}

To answer \ref{rq:defensive}, we execute \tool on four distinct binaries \wasm susceptible to three Spectre attacks. Each of the four programs is subjected to a sequence of transformations with 100 different seeds. 
We stack up to 1000 transformations for each seed. 
At each juncture of the stacking process, we assess the resulting impact of the attacks as outlined in \ref{protocol:rq3}. 
The  analysis encompasses a total of 4$\times$100$\times$1000 binaries, which also includes the original four.

\autoref{attacks:impact} offers a graphical representation of \tool's influence on the Swivel original programs and their attacks. 
Each plot corresponds to one original \wasm binary and the attack it undergoes: btb\_breakout, btb\_leakage, ret2spec, and pht.
The Y-axis represents the exfiltration bandwidth (see \autoref{metric:ber}). 
The bandwidth of the original binary under attack is marked as a blue  dashed horizontal line.
In each plot, the variants are grouped in clusters of 100 stacked transformations. 
These are indicated by green dots and lines. 
The dot signifies the median bandwidth for the cluster, while the line represents the interquartile range of the group's bandwidth.



% General results for all cases
In the instances of btb\_breakout and btb\_leakage, \tool demonstrates effectiveness, generating variants that leak less information than the original in 78\% and 70\% of the cases, respectively.
For these particular binaries, a significant reduction in exfiltration bandwidth to zero is noted following 200 stacked transformations across all 100 seeds.
As a result, with a minimum of 200 stacked transformations, \tool can create variants that are completely resistant to the original attack.
For the ret2spec and pht scenarios, the trend towards bandwidth reduction is less pronounced.
Nevertheless, the produced variants consistently exhibit lower bandwidth than the original in 76\% and 71\% of instances, respectively.
% Why is good, for breaking timers and for padding
As depicted in the plots, the exfiltration bandwidth diminishes following the application of 100 stacked transformations.

% High level of what we have seen
Our observations, articulated in response to \ref{rq:dynamic}, show that \tool fabricates variants that effectively alter memory access patterns. 
Specifically, it does so by amplifying spill/reload operations, instituting artificial global variables, and modulating the frequency of pre-existing memory accesses. 
These transformations influence the program's stack, potentially causing disruption to any cache time predictors. 
As a result, these alterations contribute to a reduction in exfiltration bandwidth, which benefits overall system security.

% More fine grained
On the other hand, many attacks rely on a timer component to measure cache access time for memory lines, and disrupting this component can effectively impair the attack's precision. 
This strategy of dynamic alteration has been purposefully employed in different scenarios. 
For instance, to counter potential timing attacks, Firefox randomizes its built-in JavaScript timer \cite{10.1007/978-3-319-70972-7_13}. \tool conveniently facilitates this strategy by interspersing instructions within the timing steps of \textit{\wasm} variants. 
In \autoref{example:timer} and \autoref{example:timer2}, we demonstrate \tool's impact on time measurements. 
The former illustrates the original time measurement, while the latter presents a variant with \tool-inserted operations amid the timing.

\input{snippets/rq3/timer}

\tool proves effective against cache access timers because the time measurement of single or a few instructions is inherently random. 
By introducing more instructions, this randomness is amplified, thereby reducing the timer's accuracy.
% Mention something about performance here

%\todo{J: TBD, I was about to add a piece of code here, but the RQ is already too large. TODO }
Furthermore, CPUs have a maximum capacity for the number of instructions they can cache.
\tool integrates instructions in such a way that the vulnerable instruction may exceed this cacheable instruction limit.
This kind of transformation can be viewed as padding \cite{padding}.
In \autoref{example:padding} and \autoref{example:padding2}, we illustrate the effect of \tool on padding instructions.
\autoref{example:padding} presents the original code used for training the branch predictor, along with the expected speculated code.
Conversely, \autoref{example:padding2} showcases a variant where \tool embeds instructions within the memory accesses.

\input{snippets/rq3/padding}

The padding alters the arrangement of the binary code in memory, effectively impeding the attacker's capacity to initiate speculative execution.
Even when an attack is launched and the vulnerable code is "speculated", the memory access is not.


In every program, we note that the exfiltration bandwidth tends to be greater than the original when the variants include a small number of transformations.
This indicates that, although the transformations generally contribute to the reduction of data leakage, the initial few might not consistently contribute positively towards this objective.
We have identified several fundamental reasons, which we discuss below.

Firstly, as emphasized in prior applications of \tool \cite{CABRERAARTEAGA2023103296}, uncontrolled diversification can be counterproductive if a specific objective, such as a cost function, is not established at the beginning of the diversification process.
%Second, \tool applies certain "symmetric" transformations.
%In particular, vulnerable code segments may be duplicated across the variants, rendering the variant more susceptible to exploitation. 
%  For later \todo{these symmetric transformations also occur with large number of transformations, so why are the variants with many transformations protected?}
Secondly, while some transformations yield distinct \wasm binaries, their compilation produces identical machine code.
Transformations that are not preserved undermine the effectiveness of diversification.
% This is to answer to the RW of Swivel itself, they proposed ot add nop operations. We know that does not work.
For example, incorporating random \texttt{nop} operations directly into \wasm does not modify the final machine code as the \texttt{nop} operations are easily removed by the standard lucet compiler.
The same phenomenon is observed with transformations to custom sections of \Wasm binaries.
Additionally, it's important to note that transformed code doesn't always execute, i.e., \tool may generate dead code.

Finally, for ret2spec and pht, both exhibit a subtle trend towards bandwidth reduction, but this does not materialize in a short-term timeframe (low count of stacked transformations) when comparing to btb\_breakout and btb\_leakage.
Furthermore,  the exfiltration bandwidth is more dispersed for these two programs.
Our analysis indicates a correlation between these trends and the complexity of the binary subject to diversification.
Ret2spec and pht are considerably larger than btb\_breakout and btb\_leakage.
The former comprises more than 300k instructions, while the latter two include fewer than 800 instructions.
Given that \tool applies precise, fine-grained transformations one at a time, the likelihood of impacting critical attack components, such as timing memory accesses, diminishes for larger binaries, particularly when limited to 1,000 transformations.
Based on these observations, we believe that a greater number of stacked transformations will further contribute to completely eliminating the attacks associated with ret2spec and pht.

% Performance impact


%\todo{It might be possible to run a test-case reducer...would it work?}

%\todo{TBD: Someone might say that the performance is affected and that is why we have less bandwidth. The idea is to answer to this in RQ2. On the other hand, I have collected the times as well, we can show the execution time distribution in clusters.}

\begin{tcolorbox}[boxrule=1pt,arc=.3em,boxsep=-1.3mm]
  \textbf{Answer to \ref{rq:defensive}}:   software diversification is effective at synthesizing \wasm binaries that are less susceptible  to Spectre attacks.  
  \tool generates variants of btb\_breakout and btb\_leakage that are totally protected and variants of ret2spec and pht that are more resilient than the original.
Larger programs require a greater number of transformations to effectively neutralize Spectre attacks.
    In general, 70\% of of the diversified variants exhibit a reduced data leakage bandwidth compared to their original programs.
\end{tcolorbox}


%\todo{The complexity of the program affects the impact. The larger the program, more difficult to target specific locations. Notions of "knobs". Measure some meta of the original binaries. Number of functions, size in bytes, number of instructions. With more instructions, more the chances to hit the incorrect location.}




%\todo{the pht attack works better. It is more resilient. The reason is related to PHT. Pattern history table attach. Elaborate on this.}


%\todo{Add a figure to show how the traces change. Use tracergraph for it...it will be easier.}


%\tool


%\subsection{\rqtesting}

%\todo{We break binaryen...that is a good takeaway.}


%\todo{Variants generated with wasm-mutate break binaryen ! This might be case 3 !} \todo{We can collect the tools for Wasm presented along all these years and break them, highlighting the need for more testing...this will be nice.}


\section{Discussion}
\label{discussion}

\emph{Preservation:} The substantial differences in code preservation between wasmtime and V8 carry notable implications. 
On one side, high preservation rates signal a possible deficiency in optimizations, as previously underlined in \cite{wasmslim}. 
Conversely, high preservation may inadvertently allow vulnerable code to persist.
For instance, although \tool can generate more resilient variants, the compiler's role in mitigating initial vulnerabilities is crucial. 
High preservation could, therefore, potentially hinder the removal of potential security weaknesses during the compilation process.


\emph{Fuzzing \Wasm compilers:}  
Identifying effective seeds for initiating fuzzing processes is a recognized challenge \cite{7958599}. 
However, programs known to be correctly compiled and executed can be diversified using \tool.
This research presents compelling evidence of preservation and variation in execution traces at an impressive scale - thousands of unique programs generated from a single original binary seed. 
As our analysis of preservation suggests, this strategy can effectively exert pressure on more compiler components, thereby enhancing the fuzzing coverage.
Thus, semantically equivalent transformations serve as a useful tool for boosting the efficacy of fuzzing techniques.


\emph{Program Normalization:} \tool was previously employed successfully for the evasion of malware detection, as outlined in \cite{CABRERAARTEAGA2023103296}. 
The proposed mitigation in the prior study involved code normalization as a means of reducing the spectrum of malware variants. 
Our current work provides insights into the potential effectiveness of this approach. 
Specifically, a practically costless process of pre-compiling Wasm binaries could be employed as a preparatory measure for malware classifiers. 
In other words, a \wasm binary can first be compiled with wasmtime, effectively eliminating nearly 40\% of malware variants according to our preservation statistics for wasmtime. 
This approach could substantially enhance the efficiency and precision of malware detection systems.





\section{Related Work}
\label{rw}

In the following text we mention relevant works in the areas of software diversification and the analysis and transformation tools for \Wasm.


% Binary code diversification, static 
\emph{Static diversification} involves the creation, synthesis, and distribution of unique, but functionally equivalent, binary files to end users. 
The pioneering work of Cohen \cite{cohen1993operating} established a suite of code transformations to achieve this static diversity. 
The primary objective of these transformations is to enhance the complexity and difficulty of launching a successful attack against a vast user base~\cite{cohen1993operating}. 
Cohen's seminal work forms the cornerstone of our approach, as most of the rewriting rules implemented in \tool draw from his contributions.
\todo{Divide the work into three categories, from source code, compiler and in the final binary.}
Jackson and colleagues~\cite{jackson2011compiler} propose that the compiler can play a crucial role in facilitating software diversification. 
They advocate for the application of numerous semantic-preserving transformations to enable large-scale software diversity, ensuring each user receives a uniquely diversified variant. 
In the context of \Wasm, 
the innovative tool for software diversification, CROW, draws heavily upon prior techniques, as acknowledged by its creators \cite{arteaga2020crow}. 
Categorized as a superdiversifier \cite{jacob2008superdiversifier}, CROW situates the LLVM compiler at the very heart of its diversification method.
However, the authors of CROW acknowledge several limitations. 
First, although incorporating the diversifier directly into the LLVM compiler is not intrinsically problematic, it does restrict the tool's scope to those \wasm binaries generated via LLVM. 
In other words, any \wasm source code without an LLVM frontend implementation is unable to benefit from CROW's functionalities.
Conversely, \tool presents the same diversification techniques as CROW, but with a more universal \wasm to \wasm solution, making it compiler agnostic. 
Significantly, \tool integrates these techniques through a generic rule engine intended for software diversification, enhancing its versatility.
Second, unlike CROW, \tool does not utilize a SMT solver to verify the generated variants, it guarantees semantically equivalence by design. 
As a result, \tool is more efficient in generating \wasm variants, as detailed in \autoref{rq:static:results}.
Finally, \tool enhances the capabilities of CROW by incorporating memory-like transformations. 
As a \wasm to \wasm diversification tool, \tool expands the coverage for tools that generate \wasm programs, as discussed throughout this work.

% For example, a custom rewriting rule could be  )
The process of diversifying a \Wasm program can be conceptualized as a three-stage procedure: parsing the program, transforming it, and finally re-encoding it back into \wasm. 
Our review of the literature has revealed several studies that have employed parsing and encoding components for \wasm binaries across various domains. 
This indicates that these works accept a \wasm binary as an input and output a unique \wasm binary. 
These domains span optimization \cite{wasmslim}, control flow \cite{10123627}, and dynamic analysis \cite{wasabi, stievenart2020compositional, 10123627, BRITO2022102745}.
When the transformation stage introduces randomized mutations to the original program, the aforementioned tools could potentially be construed as diversifiers \cite{some paper claming this}.
However, as they have been custom-built for specific tasks, these tools lack the necessary adaptability to function as all-purpose diversifiers. 
Interestingly, the other side of the coin is possible with \tool.
For instance, \tool can serve as both an optimizer and a test case reducer due to the incorporation of an e-graph at the heart of its diversification process \cite{10.1145/1480881.1480915}. 
To the best of our knowledge, the introduction of an e-graph into \tool marks the first endeavor to integrate an e-graph into a \wasm to \wasm analysis tool.



%\todo{and so what is the main conclusion here? that we could not reuse these tools for diversification? that  \tool could be used for some of these tasks? do some tools use e-graphs? is that the main novelty of \tool regarding wasm analysis?}

BREWasm \cite{rewritingtool2023} offers a comprehensive static binary rewriting framework for \Wasm and can be considered to be the most similar to \tool. 
It parses a Wasm binary into objects, rewrites them using fine-grained APIs, integrates these APIs to provide high-level ones, and re-encodes the updated objects back into a valid Wasm binary. 
The effectiveness and efficiency of BREWasm have been demonstrated through various Wasm applications and case studies on code obfuscation, software testing, program repair, and software optimization. 
%\todo{I don't understand the following 2 sentences. What are the main differences between \tool and BREWasm? in their input? in their output? in their purpose? in their technical approach? }
BREWasm implementation follows a completely different technical approach.
In comparison with our tool, the authors pointed out that our tool employs lazy parsing of Wasm. 
Although they perceived this as a limitation, it is eagerly implemented to accelerate the generation of \wasm binaries.
Additionally, our tool leverages the parser and encoder of wasmtime, a standalone compiler and interpreter for Wasm, thereby boosting its reliability and lowering its error-prone nature.
Madvex \cite{madvex}, on the other hand, alters Wasm binaries with the goal of eluding malware detectors according to a generic reward function, which involves applying transformations to the code to improve malware evasion. 
Their approach primarily focuses on modifying the code section of an input Wasm binary. 
\tool has previously been tested and proven successful for such a task \cite{CABRERAARTEAGA2023103296}, demonstrating its capacity to employ a broader range of transformations, not confined solely to the code section.





\section{Conclusion}
\label{conc}


% We summarize the three RQs
\tool exhibits robust diversification, with a 100\% diversification rate across the original 303 programs proposed by CROW. 
It creates over 9000 unique variants per program in just an hour, boasting thousands of unique machine code variants, despite preservation rates of \preserved for the wasmtime JIT compiler. 
These variants enable examination of compiler optimization impacts.
Despite non-preservation and dead code generation, \tool ensures significant diversity in execution traces. 
Moreover, \tool enhances resilience against Spectre attacks, producing fully protected variants of two versions of the btb attack, and variants of ret2spec and pht that leak less data than the original ones.


% Future work
In future works, we aim to fine-tune the diversification process, balancing broad diversification with the practical needs of real-world applications. 
The creation of rewriting rules for \tool is currently a manual task, yet we have identified potential for automation. 
For instance, \tool could be enhanced through incorporation of automatically verified rewriting rules, potentially sourced via data-driven methods such as rule mining from CROW \cite{10026577}.
Furthermore, we have observed that the impact of \tool on ret2spec and pht attacks is considerably less compared to btb attacks. 
These attacks exploit the returning address of executed functions in the program stack. 
As discussed in \ref{rq:dynamic}, \tool can generate variants that alter the return addresses of semantically equivalent functions. 
This suggests the potential for a multivariant execution strategy, similar to MEWE \cite{MEWE}, that can be implemented using \tool. 
By offering different execution paths, the returning addresses on the stack at each function execution would vary, thereby hardening binaries against ret2spec and pth attacks.
Overall, we plan striking a balance between code preservation, uniqueness of execution traces, and improved security of the generated variants.


\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\end{document}
\endinput
%%
%% End of file `sample-sigplan.tex'.


https://dl.acm.org/doi/pdf/10.1145/3547622